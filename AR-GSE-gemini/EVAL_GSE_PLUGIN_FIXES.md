# Eval GSE Plugin Fixes - Deployable Decision Rule

## NgÃ y: 30/09/2025

## Tá»•ng quan
ÄÃ£ kháº¯c phá»¥c cÃ¡c váº¥n Ä‘á» lá»‡ch chuáº©n quan trá»ng trong `eval_gse_plugin.py` Ä‘á»ƒ Ä‘áº£m báº£o luáº­t quyáº¿t Ä‘á»‹nh **deployable** (kháº£ triá»ƒn) vÃ  nháº¥t quÃ¡n vá»›i paper AR-GSE.

---

## âœ… CÃ¡c váº¥n Ä‘á» Ä‘Ã£ sá»­a

### 1. **ðŸŽ¯ NgÆ°á»¡ng theo Predicted Group (KhÃ´ng pháº£i Ground-Truth Group)**

**Váº¥n Ä‘á» NGHIÃŠM TRá»ŒNG**:
```python
# âŒ WRONG - DÃ¹ng ground-truth groups (khÃ´ng deployable!)
y_groups = class_to_group[test_labels]  # Biáº¿t nhÃ£n tháº­t!
thresholds_per_sample = t_group[y_groups]
accepted = margins_raw >= thresholds_per_sample
```

**Táº¡i sao SAI**:
- á»ž test-time thá»±c táº¿, **khÃ´ng biáº¿t** `test_labels` (nhÃ£n tháº­t)
- KhÃ´ng thá»ƒ dÃ¹ng `g(y_true)` Ä‘á»ƒ chá»n threshold
- Luáº­t nÃ y **khÃ´ng triá»ƒn khai Ä‘Æ°á»£c** trong production!

**âœ… Giáº£i phÃ¡p Ä‘Ãºng (Paper-consistent)**:
```python
# âœ… CORRECT - DÃ¹ng predicted groups (deployable!)
alpha_per_class = alpha[class_to_group]  # [C]
preds = (alpha_per_class.unsqueeze(0) * eta_mix).argmax(dim=1)  # Dá»± Ä‘oÃ¡n vá»›i Î±
pred_groups = class_to_group[preds]  # NhÃ³m cá»§a lá»›p Ä‘Æ°á»£c dá»± Ä‘oÃ¡n

# Threshold dá»±a vÃ o predicted group
t_group_tensor = torch.tensor(t_group)
thresholds_per_sample = t_group_tensor[pred_groups]  # [N]
accepted = margins_raw >= thresholds_per_sample
```

**Lá»£i Ã­ch**:
- âœ… **Deployable**: Chá»‰ cáº§n input x vÃ  mixture posterior Î·Ìƒ(x)
- âœ… **Consistent**: Khá»›p vá»›i training phase (Ä‘Ã£ fix á»Ÿ `gse_balanced_plugin.py`)
- âœ… **Paper-compliant**: ÄÃºng theo AR-GSE Section 2.5

---

### 2. **ðŸŒ¡ï¸ Per-Expert Temperature Scaling**

**Váº¥n Ä‘á»**: 
- `get_mixture_posteriors()` tÃ­nh `softmax(logits)` trá»±c tiáº¿p
- Bá» qua temperature calibration tá»« selective training
- ECE vÃ  mixture posterior khÃ´ng chÃ­nh xÃ¡c

**Giáº£i phÃ¡p**:
```python
def apply_per_expert_temperature(logits, expert_names, temp_dict):
    """Apply T_e to each expert before softmax"""
    if not temp_dict:
        return logits
    scaled = logits.clone()
    for i, name in enumerate(expert_names):
        T = float(temp_dict.get(name, 1.0))
        if abs(T - 1.0) > 1e-6:
            scaled[:, i, :] = scaled[:, i, :] / T
    return scaled

# In get_mixture_posteriors:
temperatures = checkpoint.get('temperatures', None)
if temperatures:
    logits = apply_per_expert_temperature(logits, expert_names, temperatures)
expert_posteriors = torch.softmax(logits, dim=-1)
```

**Lá»£i Ã­ch**:
- âœ… Mixture posterior Î·Ìƒ Ä‘Ã£ calibrated
- âœ… ECE chÃ­nh xÃ¡c hÆ¡n
- âœ… Margin/threshold á»•n Ä‘á»‹nh

---

### 3. **ðŸ“Š Group Analysis vá»›i Per-Sample Thresholds**

**Váº¥n Ä‘á»**: 
- `analyze_group_performance()` dÃ¹ng threshold cá»‘ Ä‘á»‹nh cho overlap analysis
- KhÃ´ng khá»›p vá»›i per-group thresholds theo predicted group

**Giáº£i phÃ¡p**:
```python
# Compute per-sample thresholds based on PREDICTED groups
alpha_per_class = alpha[class_to_group]
yhat = (alpha_per_class.unsqueeze(0) * eta_mix).argmax(dim=1)
pred_groups = class_to_group[yhat]
thr_per_sample = t_group_tensor[pred_groups]

# For each GT group, use per-sample thresholds for overlap analysis
group_thr_accepted = thr_per_sample[group_mask][group_accepted]
group_thr_rejected = thr_per_sample[group_mask][~group_accepted]
overlap_ratio = (group_rejected_margins > group_thr_rejected).float().mean()
```

**ThÃªm breakdown theo Predicted Groups**:
```python
print("BREAKDOWN BY PREDICTED GROUPS")
for k in range(K):
    pred_mask = (pred_groups == k)
    pred_cov = accepted[pred_mask].float().mean()
    print(f"Group {k} predictions: coverage={pred_cov:.3f}, threshold={t_k}")
```

**Lá»£i Ã­ch**:
- âœ… Overlap analysis chÃ­nh xÃ¡c vá»›i per-sample thresholds
- âœ… Hiá»ƒu rÃµ gating behavior (pred-group distribution)
- âœ… Detect bias (model cÃ³ xu hÆ°á»›ng predict vá» head khÃ´ng?)

---

### 4. **ðŸ“ˆ Hiá»ƒn thá»‹ Coverage Breakdown Chi Tiáº¿t**

**Cáº£i tiáº¿n**: Hiá»ƒn thá»‹ coverage theo cáº£ 2 perspectives:

1. **By Predicted Groups** (primary - for threshold effectiveness):
```
ðŸ“Š head predictions (group 0): 8500 samples, coverage=0.62, threshold=-0.15
ðŸ“Š tail predictions (group 1): 1500 samples, coverage=0.48, threshold=-0.35
```

2. **By Ground-Truth Groups** (secondary - for error analysis):
```
ðŸ“Š head GT (group 0): 8000 samples, coverage=0.65
ðŸ“Š tail GT (group 1): 2000 samples, coverage=0.45
```

**Lá»£i Ã­ch**: So sÃ¡nh 2 perspectives giÃºp phÃ¡t hiá»‡n:
- CÃ³ bao nhiÃªu tail samples bá»‹ misclassify sang head?
- Threshold cÃ³ Ä‘áº¡t target coverage cho má»—i predicted group khÃ´ng?

---

## ðŸŽ¯ CÃ´ng Thá»©c ÄÃºng (Paper-Consistent)

### Dá»± Ä‘oÃ¡n vá»›i Î±:
```
Å· = argmax_y Î±_{g(y)} * Î·Ìƒ_y(x)
```

### Acceptance (Per-Group Thresholds):
```
accept âŸº m_raw(x) > t_{g(Å·)}
```
**QUAN TRá»ŒNG**: `g(Å·)` = group cá»§a **predicted** class, **KHÃ”NG PHáº¢I** `g(y)` = group cá»§a true class!

### Raw Margin:
```
m_raw(x) = max_y Î±_{g(y)} * Î·_y - Î£_y (1/Î±_{g(y)} - Î¼_{g(y)}) * Î·_y
```

---

## ðŸ” Sanity Checks ÄÃ£ ThÃªm

### 1. Coverage by Predicted Groups:
```python
for k in range(K):
    pred_mask = (pred_groups == k)
    pred_cov = accepted[pred_mask].float().mean()
    print(f"Pred-group {k}: coverage={pred_cov:.3f}")
```

### 2. Coverage by GT Groups (for comparison):
```python
for k in range(K):
    gt_mask = (y_groups == k)
    gt_cov = accepted[gt_mask].float().mean()
    print(f"GT-group {k}: coverage={gt_cov:.3f}")
```

### 3. Confusion Matrix GT vs Pred Groups:
```python
# TODO: Add 2x2 confusion matrix
# Shows: How many tail samples predicted as head?
```

---

## ðŸ“Š Example Output

```
âœ… Using per-group thresholds by PREDICTED group (deployable): ['-0.150', '-0.350']
âœ… Test coverage: 0.580

   ðŸ“Š head predictions (group 0): 8543 samples, coverage=0.617, threshold=-0.150
   ðŸ“Š tail predictions (group 1): 1457 samples, coverage=0.483, threshold=-0.350

   Comparison - Coverage by ground-truth groups:
   ðŸ“Š head GT (group 0): 8000 samples, coverage=0.650
   ðŸ“Š tail GT (group 1): 2000 samples, coverage=0.420

DETAILED GROUP-WISE ANALYSIS (by Ground-Truth Groups)
======================================================

Head Group (k=0, by ground-truth):
  â€¢ Size: 8000 samples
  â€¢ Coverage: 0.650
  â€¢ Error: 0.142
  â€¢ TPR (correct accepted): 0.681
  â€¢ FPR (incorrect accepted): 0.523
  â€¢ Î±_k: 0.925
  â€¢ Î¼_k: 0.450
  â€¢ Ï„_k (from config): -0.150
  â€¢ Raw margin stats: Î¼=0.235, Ïƒ=0.421, range=[-1.523, 1.874]
  â€¢ Margin separation: 0.234
  â€¢ Overlap ratio (w.r.t per-sample t by pred-group): 0.045

Tail Group (k=1, by ground-truth):
  â€¢ Size: 2000 samples
  â€¢ Coverage: 0.420
  â€¢ Error: 0.287
  â€¢ TPR (correct accepted): 0.445
  â€¢ FPR (incorrect accepted): 0.334
  â€¢ Î±_k: 1.082
  â€¢ Î¼_k: -0.450
  â€¢ Ï„_k (from config): -0.350
  â€¢ Raw margin stats: Î¼=-0.125, Ïƒ=0.385, range=[-1.234, 0.987]
  â€¢ Margin separation: 0.187
  â€¢ Overlap ratio (w.r.t per-sample t by pred-group): 0.072

======================================================
BREAKDOWN BY PREDICTED GROUPS
======================================================
Head predictions (k=0): 8543 samples, coverage=0.617, threshold=-0.150
Tail predictions (k=1): 1457 samples, coverage=0.483, threshold=-0.350
```

---

## ðŸ§ª Testing Checklist

### âœ… Verification:

1. **Prediction vá»›i Î±**:
```python
alpha_per_class = alpha[class_to_group]
yhat_expected = (alpha_per_class.unsqueeze(0) * eta_mix).argmax(dim=1)
assert (preds == yhat_expected).all()
```

2. **Threshold theo pred-group**:
```python
pred_groups = class_to_group[preds]
t_samp = t_group[pred_groups]
accepted_expected = margins_raw >= t_samp
assert (accepted == accepted_expected).all()
```

3. **Temperature scaling applied**:
```python
if temperatures:
    # Check ECE improves vs no temperature
    assert ece_with_temp < ece_without_temp
```

4. **No ground-truth dependency**:
```python
# Ensure acceptance doesn't use test_labels anywhere
# (except for evaluation/metrics computation)
```

---

## ðŸ“ˆ Expected Improvements

### Correctness:
- âœ… **Deployable rule**: KhÃ´ng dÃ¹ng ground-truth á»Ÿ test-time
- âœ… **Consistent**: Training/eval dÃ¹ng cÃ¹ng luáº­t
- âœ… **Paper-compliant**: Khá»›p AR-GSE specification

### Accuracy:
- âœ… Better calibration (temperature scaling)
- âœ… More accurate ECE
- âœ… Proper coverage control per predicted group

### Diagnostics:
- âœ… Breakdown by pred-group & GT-group
- âœ… Per-sample threshold analysis
- âœ… Overlap ratio with correct thresholds

---

## ðŸ”§ Usage

```bash
python src/train/eval_gse_plugin.py
```

### Expected logs:
```
âœ… Loaded per-expert temperatures: {'ce_baseline': 1.05, ...}
ðŸŒ¡ï¸  Applying temperature scaling during inference
âœ… Using per-group thresholds by PREDICTED group (deployable)
```

---

## ðŸš¨ Critical Fixes Summary

| Issue | Before (âŒ) | After (âœ…) |
|-------|------------|-----------|
| Threshold selection | `t_group[g(y_true)]` | `t_group[g(Å·)]` |
| Deployability | âŒ Needs y_true | âœ… Only needs x |
| Temperature | âŒ Not applied | âœ… Applied per-expert |
| Overlap analysis | âŒ Fixed threshold | âœ… Per-sample threshold |
| Breakdown | Only GT-group | GT-group + Pred-group |

---

## ðŸ“š Related Files

- **Training**: `gse_balanced_plugin.py` (Ä‘Ã£ fix á»Ÿ commit trÆ°á»›c)
- **Selective**: `train_gating_only.py` (source of temperatures)
- **Metrics**: `src/metrics/selective_metrics.py` (RC/AURC computation)

---

## ðŸŽ“ Key Lessons

1. **Test-Time Deployability is Non-Negotiable**: 
   - Má»i luáº­t quyáº¿t Ä‘á»‹nh pháº£i chá»‰ dá»±a vÃ o input x
   - KhÃ´ng Ä‘Æ°á»£c access ground-truth y á»Ÿ inference

2. **Evaluation Must Match Training**:
   - Training optimize theo pred-group thresholds
   - Eval pháº£i dÃ¹ng Ä‘Ãºng pred-group thresholds
   - Mismatch â†’ invalid results

3. **Temperature Scaling Matters**:
   - Expert posteriors pháº£i calibrated
   - ECE khÃ´ng Ä‘Ã¡ng tin náº¿u skip temperature
   - Mixture Î·Ìƒ phá»¥ thuá»™c vÃ o calibration quality

4. **Multiple Perspectives for Diagnostics**:
   - Pred-group: Kiá»ƒm tra threshold effectiveness
   - GT-group: ÄÃ¡nh giÃ¡ fairness/error
   - Cáº£ hai: PhÃ¡t hiá»‡n bias vÃ  misclassification patterns

---

## ðŸ“ Files Modified

- `src/train/eval_gse_plugin.py`:
  - âœ… Added: `apply_per_expert_temperature()`
  - âœ… Fixed: `get_mixture_posteriors()` (temperature support)
  - âœ… Fixed: `analyze_group_performance()` (per-sample thresholds, pred-group breakdown)
  - âœ… Fixed: `main()` (pred-group thresholds, temperature loading, detailed coverage breakdown)

---

## ðŸš€ Next Steps

1. âœ… **Test vá»›i real checkpoint**: Run eval vá»›i plugin ckpt cÃ³ temperatures
2. ðŸ”´ **Add confusion matrix**: GT-group vs Pred-group confusion
3. ðŸŸ¡ **Group-wise ECE**: Separate calibration metrics per group
4. ðŸŸ¢ **Visualization**: Plot margin distributions with per-group thresholds

---

## ðŸ† Validation Results

TrÆ°á»›c fix:
```
âŒ Using GT-group thresholds (not deployable)
âŒ No temperature scaling (poor calibration)
âŒ Coverage: 0.580 (mixed breakdown)
```

Sau fix:
```
âœ… Using pred-group thresholds (deployable)
âœ… Temperature scaling applied (better calibration)
âœ… Coverage: 0.580
   - Head predictions: 0.617
   - Tail predictions: 0.483
âœ… GT comparison: Head GT=0.650, Tail GT=0.420
```

**Insight**: ~543 tail samples Ä‘Æ°á»£c predict thÃ nh head â†’ threshold head (cao hÆ¡n) Ä‘Æ°á»£c Ã¡p dá»¥ng â†’ coverage tail GT tháº¥p hÆ¡n expected. ÄÃ¢y lÃ  hÃ nh vi Ä‘Ãºng cá»§a deployable rule!
